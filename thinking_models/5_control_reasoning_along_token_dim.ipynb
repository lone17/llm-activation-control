{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb73565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46491074",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"Qwen/Qwen3-4B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These constants must match with the ones in get_activations.py\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "UPPERBOUND_MAX_NEW_TOKENS = 7000\n",
    "\n",
    "# Just hardcode to avoid loading the model\n",
    "start_thinking_token_id = 151667\n",
    "end_thinking_token_id = 151668 # Qwen3's end of thinking token id\n",
    "n_layers = 36\n",
    "# n_layers = model.config.num_hidden_layers\n",
    "n_activations = 2\n",
    "d_model = 2560"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e443077",
   "metadata": {},
   "source": [
    "# Load pre-computed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c22a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all token activations\n",
    "# ADD THE PATH TO ACTIVATIONS HERE\n",
    "activation_path = Path(\"outputs/sequences_activations.pt\")\n",
    "assert activation_path.exists()\n",
    "\n",
    "# Load the activations\n",
    "sequences_activations = torch.load(activation_path)\n",
    "assert len(sequences_activations) == TRAIN_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find (again) the positions of the start and end of thinking tokens\n",
    "\n",
    "start_thinking_positions = []\n",
    "end_thinking_positions = []\n",
    "for seq in sequences_activations:\n",
    "    start_thinking_positions.append((seq[\"generated_token_ids\"][0] == start_thinking_token_id).nonzero(as_tuple=True)[0].item())\n",
    "    end_thinking_positions.append((seq[\"generated_token_ids\"][0] == end_thinking_token_id).nonzero(as_tuple=True)[0].item())\n",
    "\n",
    "print(start_thinking_positions)\n",
    "print(end_thinking_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chosen direction\n",
    "chosen_direction = torch.load(\"outputs/chosen_direction.pt\")\n",
    "assert chosen_direction.shape == (d_model,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b45675",
   "metadata": {},
   "source": [
    "# Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc94db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_thinking_caches = []\n",
    "for i, seq in enumerate(sequences_activations):\n",
    "    token_activations = seq[\"token_activations\"]\n",
    "    caches = {}\n",
    "    for module_path, activations in token_activations.items():\n",
    "        caches[module_path] = activations[0, start_thinking_positions[i]:end_thinking_positions[i]+1, :] # (n_tokens, d_model)\n",
    "    batch_thinking_caches.append(caches)\n",
    "\n",
    "assert len(batch_thinking_caches) == TRAIN_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(batch_thinking_caches[0][\"model.layers.0.input_layernorm\"].shape) # (n_tokens, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df8545",
   "metadata": {},
   "source": [
    "### Visualization: Project token activations at an extraction point onto steering vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3389d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_path(\n",
    "    layer: int,\n",
    "    module_name: list[str] = [\"input_layernorm\", \"post_attention_layernorm\"]\n",
    "):\n",
    "    return f\"model.layers.{layer}.{module_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_extraction_point = 40\n",
    "chosen_sample_idx = 5 # Select 1 sample from the batch\n",
    "\n",
    "chosen_layer = chosen_extraction_point // 2\n",
    "chosen_module_name = \"input_layernorm\" if chosen_extraction_point % 2 == 0 else \"post_attention_layernorm\"\n",
    "\n",
    "chosen_module_path = get_module_path(chosen_layer, chosen_module_name)\n",
    "chosen_module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, n_tokens, d_model)\n",
    "batch_thinking_token_activations = [seq[chosen_module_path] for seq in batch_thinking_caches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2950f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_map = {\n",
    "    \"positive\": plotly.colors.qualitative.Plotly[0],\n",
    "    \"negative\": plotly.colors.qualitative.Plotly[1],\n",
    "    \"neutral\": plotly.colors.qualitative.Pastel1[3],\n",
    "\n",
    "}\n",
    "colour_map_light = {\n",
    "    \"positive\": plotly.colors.qualitative.Pastel1[1],\n",
    "    \"negative\": plotly.colors.qualitative.Pastel1[0],\n",
    "    \"neutral\": plotly.colors.qualitative.Pastel1[3],\n",
    "}\n",
    "colour_map_opaque = {\n",
    "    \"positive\": \"rgba(251, 180, 174, 0.3)\",\n",
    "    \"negative\": \"rgba(179, 205, 227, 0.3)\",   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269926c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = sum([[f\"{i}\", f\"{i}-post\"] for i in range(n_layers)], [])\n",
    "layer_names = [str(i) for i in range(2 * n_layers)]\n",
    "\n",
    "# dim\n",
    "activations = batch_thinking_token_activations[chosen_sample_idx] # (seq_len, d_model)\n",
    "\n",
    "# layers x resid_modules x batch_size\n",
    "scalar_projections = einops.einsum(\n",
    "    activations.to(chosen_direction.dtype),\n",
    "    chosen_direction,\n",
    "    \"... seq_len dim, ... dim -> ... seq_len\",\n",
    ")\n",
    "scalar_projections = np.nan_to_num(scalar_projections)\n",
    "\n",
    "print(\"Summary statistics of scalar projections:\")\n",
    "print(f\"Min: {scalar_projections.min()}\")\n",
    "print(f\"Max: {scalar_projections.max()}\")\n",
    "print(f\"Mean: {scalar_projections.mean()}\")\n",
    "print(f\"Median: {np.median(scalar_projections)}\")\n",
    "print(f\"Standard Deviation: {scalar_projections.std()}\")\n",
    "\n",
    "degrees = np.rad2deg(np.arccos(scalar_projections))\n",
    "\n",
    "y_values = scalar_projections.flatten()\n",
    "\n",
    "batch_size = scalar_projections.shape[-1]\n",
    "\n",
    "# x_values_flatten = sum(\n",
    "#     [\n",
    "#         [f\"{l}-mid\"] * batch_size + [f\"{l}-post\"] * batch_size\n",
    "#         for l in range(num_layers)\n",
    "#     ],\n",
    "#     [],\n",
    "# )\n",
    "# x_values = sum([[f\"{l}\", f\"{l}-post\"] for l in range(n_layers)], [])\n",
    "# x_values = [str(i) for i in range(2 * n_layers)]\n",
    "\n",
    "x_values = [str(i) for i in range(y_values.shape[0])]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_values,\n",
    "        y=y_values,\n",
    "        mode=\"lines+markers\",\n",
    "        yaxis=\"y\",\n",
    "        marker_color=colour_map_light[\"neutral\"],\n",
    "        showlegend=False,\n",
    "        marker_size=1,\n",
    "    )\n",
    ")\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=layer_names[::],\n",
    "#         y=mean_cosine[::],\n",
    "#         mode=\"markers\",\n",
    "#         yaxis=\"y\",\n",
    "#         marker_color=colour_map[\"neutral\"],\n",
    "#         showlegend=False,\n",
    "#         marker_size=1,\n",
    "#     )\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b332eea",
   "metadata": {},
   "source": [
    "### Visualization: Project mean token activations of all extraction points onto steering vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_thinking_token_activations.shape # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5028cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack\n",
    "\n",
    "batch_thinking_caches_stacked = []\n",
    "for sample in batch_thinking_caches:\n",
    "    stack = torch.stack(list(sample.values()), dim=0) # (n_layers * n_activations, n_tokens, d_model)\n",
    "    batch_thinking_caches_stacked.append(stack)\n",
    "\n",
    "batch_thinking_caches_stacked[0].shape # (n_layers * n_activations, n_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ce0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_thinking_caches_stacked_mean = [batch_thinking_caches_stacked[i].mean(dim=0) for i in range(len(batch_thinking_caches_stacked))] # (n_tokens, d_model)\n",
    "batch_thinking_caches_stacked_mean[0].shape # (n_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71461805",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_sample_idx = 5\n",
    "\n",
    "# dim\n",
    "activations = batch_thinking_caches_stacked_mean[chosen_sample_idx] # (seq_len, d_model)\n",
    "\n",
    "# layers x resid_modules x batch_size\n",
    "scalar_projections = einops.einsum(\n",
    "    activations.to(chosen_direction.dtype),\n",
    "    chosen_direction,\n",
    "    \"... seq_len dim, ... dim -> ... seq_len\",\n",
    ")\n",
    "scalar_projections = np.nan_to_num(scalar_projections)\n",
    "\n",
    "print(\"Summary statistics of scalar projections:\")\n",
    "print(f\"Min: {scalar_projections.min()}\")\n",
    "print(f\"Max: {scalar_projections.max()}\")\n",
    "print(f\"Mean: {scalar_projections.mean()}\")\n",
    "print(f\"Median: {np.median(scalar_projections)}\")\n",
    "print(f\"Standard Deviation: {scalar_projections.std()}\")\n",
    "\n",
    "degrees = np.rad2deg(np.arccos(scalar_projections))\n",
    "\n",
    "y_values = scalar_projections.flatten()\n",
    "\n",
    "batch_size = scalar_projections.shape[-1]\n",
    "\n",
    "# x_values_flatten = sum(\n",
    "#     [\n",
    "#         [f\"{l}-mid\"] * batch_size + [f\"{l}-post\"] * batch_size\n",
    "#         for l in range(num_layers)\n",
    "#     ],\n",
    "#     [],\n",
    "# )\n",
    "# x_values = sum([[f\"{l}\", f\"{l}-post\"] for l in range(n_layers)], [])\n",
    "# x_values = [str(i) for i in range(2 * n_layers)]\n",
    "\n",
    "x_values = [str(i) for i in range(y_values.shape[0])]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_values,\n",
    "        y=y_values,\n",
    "        mode=\"lines+markers\",\n",
    "        yaxis=\"y\",\n",
    "        marker_color=colour_map_light[\"neutral\"],\n",
    "        showlegend=False,\n",
    "        marker_size=1,\n",
    "    )\n",
    ")\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=layer_names[::],\n",
    "#         y=mean_cosine[::],\n",
    "#         mode=\"markers\",\n",
    "#         yaxis=\"y\",\n",
    "#         marker_color=colour_map[\"neutral\"],\n",
    "#         showlegend=False,\n",
    "#         marker_size=1,\n",
    "#     )\n",
    "# )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
