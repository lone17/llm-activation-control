{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc97d59",
   "metadata": {},
   "source": [
    "```md\n",
    "positive - thinking:\n",
    "\tGive me a short introduction to large language model<think>...</think>\\n\\n\n",
    "\t\t\t\t\t\t\t\t    ^\n",
    "\t\t\t\t\t\t\t    select token 80th here\n",
    "negative - no-thinking: \n",
    "\tGive me a short introduction to large language model<think>\\n\\n</think>\\n\\n\n",
    "\t\t\t\t\t\t\t\t\t\t^\n",
    "\t\t\t\t\t\t\t\t\t\there\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab865ca",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama kaleido numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from jaxtyping import Float, Int\n",
    "from enum import StrEnum\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Qwen/Qwen3-4B\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float16,\n",
    "    default_padding_side='left',\n",
    "    # fp16=True\n",
    ")\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "# model.tokenizer.pad_token = '<|extra_0|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_instructions_qwen_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: list[str],\n",
    "    enable_thinking: bool = True,\n",
    ") -> Int[Tensor, \"batch_size seq_len\"]:\n",
    "    prompts = []\n",
    "    for instruction in instructions:\n",
    "        message = [{\"role\": \"user\", \"content\": instruction}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "            enable_thinking=enable_thinking,\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_enable_thinking_fn = functools.partial(\n",
    "    tokenize_instructions_qwen_chat,\n",
    "    tokenizer=model.tokenizer,\n",
    "    enable_thinking=True,\n",
    ")\n",
    "\n",
    "tokenize_instructions_disable_thinking_fn = functools.partial(\n",
    "    tokenize_instructions_qwen_chat,\n",
    "    tokenizer=model.tokenizer,\n",
    "    enable_thinking=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_instructions() -> tuple[list[str], list[str]]:\n",
    "    url = \"https://raw.githubusercontent.com/cvenhoff/steering-thinking-llms/refs/heads/main/messages/messages.py\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    # Save to file\n",
    "    with open(\"messages.py\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    # Load the messages\n",
    "    assert Path(\"messages.py\").exists()\n",
    "    from messages import messages, eval_messages\n",
    "\n",
    "    train_contents = [msg[\"content\"] for msg in messages]\n",
    "    eval_contents = [msg[\"content\"] for msg in eval_messages]\n",
    "\n",
    "    # Shuffle the messages\n",
    "    random.shuffle(train_contents)\n",
    "    random.shuffle(eval_contents)\n",
    "\n",
    "    return train_contents, eval_contents\n",
    "\n",
    "instructions_train, instructions_test = get_dataset_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a962313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"instructions_train: {len(instructions_train)}\")\n",
    "print(f\"instructions_test: {len(instructions_test)}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"instructions_train:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(instructions_train[i])}\")\n",
    "print(\"instructions_test:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(instructions_test[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f59c45",
   "metadata": {},
   "source": [
    "## Compute activations & refusal dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationName(StrEnum):\n",
    "    RESID_PRE = \"resid_pre\"\n",
    "    RESID_MID = \"resid_mid\"\n",
    "    RESID_POST = \"resid_post\"\n",
    "\n",
    "\n",
    "required_activations = [\n",
    "    ActivationName.RESID_MID,\n",
    "    ActivationName.RESID_POST,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d373862",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_BATCHES = 10\n",
    "MAX_NEW_TOKENS = 80\n",
    "\n",
    "start_thinking_token_id = 151667\n",
    "end_thinking_token_id = 151668 # Qwen3's end of thinking token id\n",
    "n_layers = model.cfg.n_layers\n",
    "n_activations = 2\n",
    "d_model = model.cfg.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e38b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_generation_step(\n",
    "    model: \"HookedTransformer\",\n",
    "    tokens: Tensor,\n",
    "    max_new_tokens: int = 10,\n",
    "    activation_types: list[str] = [\"resid_mid\", \"resid_post\"],\n",
    "    offload_to_cpu: bool = True,\n",
    "    early_stop_at_end_thinking_token: bool = True,\n",
    ") -> tuple[dict[tuple[str, int], Tensor], Tensor]:\n",
    "    \"\"\"Get activations during generation for the last token position at each step.\n",
    "    \"\"\"\n",
    "    batch_size = tokens.shape[0]\n",
    "    d_model = model.cfg.d_model\n",
    "    n_layers = model.cfg.n_layers\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    \n",
    "    # Pre-allocate tensors for efficiency\n",
    "    all_step_caches: dict[tuple[str, int], Tensor] = {}\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    for act_type in activation_types:\n",
    "        for layer in range(n_layers):\n",
    "            all_step_caches[(act_type, layer)] = torch.zeros(\n",
    "                batch_size, max_new_tokens, d_model, \n",
    "                device=\"cpu\" if offload_to_cpu else tokens.device,\n",
    "                dtype=model_dtype\n",
    "            )\n",
    "    \n",
    "    for step in tqdm(range(max_new_tokens), desc=\"Generating\"):\n",
    "        # Get activations for current sequence\n",
    "        logits, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Store last token activations for this step\n",
    "        for layer in range(n_layers):\n",
    "            for act_type in activation_types:\n",
    "                cache_key = (act_type, layer)\n",
    "                last_token_activation = cache[act_type, layer][:, -1, :].to(model_dtype) # (batch_size, d_model)\n",
    "                if offload_to_cpu:\n",
    "                    last_token_activation = last_token_activation.cpu()\n",
    "                all_step_caches[cache_key][:, step, :] = last_token_activation\n",
    "\n",
    "        # Generate next token(s), currently only support greedy generation\n",
    "        if batch_size == 1:\n",
    "            # Single sequence generation\n",
    "            next_token = logits[0, -1].argmax()\n",
    "            if early_stop_at_end_thinking_token:\n",
    "                if next_token == end_thinking_token_id:\n",
    "                    break\n",
    "            \n",
    "            tokens = torch.cat([tokens, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        else:\n",
    "            # Multi-batch generation\n",
    "            next_tokens = logits.argmax(dim=-1)[:, -1]  # [batch_size]\n",
    "            if early_stop_at_end_thinking_token:\n",
    "                if torch.all(next_tokens == end_thinking_token_id):\n",
    "                    break\n",
    "\n",
    "            tokens = torch.cat([tokens, next_tokens.unsqueeze(1)], dim=1)\n",
    "        \n",
    "\n",
    "    return all_step_caches, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    instruction: str\n",
    "    is_thinking: bool\n",
    "    caches: Tensor\n",
    "    tokens: Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = instructions_train[:TRAIN_BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b792047",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_thinking = []\n",
    "for i, instruction in enumerate(instructions):\n",
    "    print(f\"Processing instruction {i}:\")\n",
    "    tokens = tokenize_instructions_enable_thinking_fn(instructions=[instruction])\n",
    "    print(f\"Number of tokens: {tokens.shape[1]}\")\n",
    "    caches, tokens = get_activations_generation_step(\n",
    "        model,\n",
    "        tokens.to(DEVICE),\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        activation_types=[\"resid_mid\", \"resid_post\"]\n",
    "    )\n",
    "\n",
    "    batch_thinking.append(\n",
    "        Sample(\n",
    "            caches=caches,\n",
    "            tokens=tokens,\n",
    "            instruction=instruction,\n",
    "            is_thinking=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54121f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(batch_thinking) == TRAIN_BATCH_SIZE\n",
    "print(model.tokenizer.decode(batch_thinking[0].tokens[0]))\n",
    "print(batch_thinking[0].caches[\"resid_mid\", 0].shape) # (max_new_tokens, d_model)\n",
    "print(batch_thinking[0].caches[\"resid_post\", 0].shape) # (max_new_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No-thinking caches\n",
    "# In this case, max_new_tokens = 4\n",
    "\n",
    "batch_no_thinking = []\n",
    "for instruction in instructions:\n",
    "    no_thinking_tokens = tokenize_instructions_disable_thinking_fn(instructions=[instruction])\n",
    "\n",
    "    _, no_thinking_cache = model.run_with_cache(\n",
    "        no_thinking_tokens,\n",
    "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
    "        return_cache_object=True\n",
    "    )\n",
    "\n",
    "    # Only get the caches at reasoning tokens (before </think>) -> '<think>' and '/n/n'\n",
    "    sub_cache = {}\n",
    "    for activation_type in [\"resid_mid\", \"resid_post\"]:\n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            sub_cache[activation_type, layer] = no_thinking_cache[activation_type, layer][:,-4:,:] \n",
    "    \n",
    "    batch_no_thinking.append(Sample(\n",
    "        caches=sub_cache,\n",
    "        tokens=no_thinking_tokens,\n",
    "        instruction=instruction,\n",
    "        is_thinking=False,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(batch_no_thinking) == TRAIN_BATCH_SIZE\n",
    "print(batch_no_thinking[0].caches[\"resid_mid\", 0].shape) # (num_last_tokens, d_model)\n",
    "print(batch_no_thinking[0].caches[\"resid_post\", 0].shape) # (num_last_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_token_pos = -1\n",
    "num_last_tokens = abs(chosen_token_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be588d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_mean_activations = torch.zeros(\n",
    "    n_layers, n_activations, num_last_tokens, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, num_last_tokens, d_model)\n",
    "\n",
    "thinking_activations_all = torch.zeros(\n",
    "    n_layers, n_activations, TRAIN_BATCH_SIZE, num_last_tokens, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, batch_size, num_last_tokens, d_model)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    batch_thinking_activations = []\n",
    "    for act in [\"resid_mid\", \"resid_post\"]:\n",
    "        # Get mean activations across tokens dimension\n",
    "        batch_thinking_activations_per_act = torch.stack([\n",
    "            sample.caches[act, layer][0, -num_last_tokens:, :] for sample in batch_thinking\n",
    "        ]) # (batch_size, num_last_tokens, d_model)\n",
    "        batch_thinking_activations.append(batch_thinking_activations_per_act)\n",
    "    batch_thinking_activations = torch.stack(batch_thinking_activations) # (n_activations, batch_size, num_last_tokens, d_model)\n",
    "\n",
    "    # Normalize then get mean because the activation will be normalized by the RMSNorm layer\n",
    "    batch_thinking_activations = batch_thinking_activations / batch_thinking_activations.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute mean across batch dimension\n",
    "    thinking_mean_activations[layer] = batch_thinking_activations.mean(dim=1)\n",
    "    thinking_activations_all[layer] = batch_thinking_activations\n",
    "\n",
    "print(thinking_mean_activations.shape) # (n_layers, n_activations, num_last_tokens, d_model)\n",
    "print(thinking_activations_all.shape) # (n_layers, n_activations, batch_size, num_last_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_thinking_mean_activations = torch.zeros(\n",
    "    n_layers, n_activations, num_last_tokens, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, num_last_tokens, d_model)\n",
    "\n",
    "no_thinking_activations_all = torch.zeros(\n",
    "    n_layers, n_activations, TRAIN_BATCH_SIZE, num_last_tokens, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, batch_size, num_last_tokens, d_model)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    batch_no_thinking_activations = []\n",
    "    for act in [\"resid_mid\", \"resid_post\"]:\n",
    "        # Get mean activations across tokens dimension\n",
    "        batch_no_thinking_activations_per_act = torch.stack([\n",
    "            sample.caches[act, layer][0, -num_last_tokens:, :] for sample in batch_no_thinking\n",
    "        ]) # (batch_size, num_last_tokens, d_model)\n",
    "        batch_no_thinking_activations.append(batch_no_thinking_activations_per_act)\n",
    "    batch_no_thinking_activations = torch.stack(batch_no_thinking_activations) # (n_activations, batch_size, num_last_tokens, d_model)\n",
    "\n",
    "    # Normalize then get mean because the activation will be normalized by the RMSNorm layer\n",
    "    batch_no_thinking_activations = batch_no_thinking_activations / batch_no_thinking_activations.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute mean across batch dimension\n",
    "    no_thinking_mean_activations[layer] = batch_no_thinking_activations.mean(dim=1)\n",
    "    no_thinking_activations_all[layer] = batch_no_thinking_activations\n",
    "\n",
    "print(no_thinking_mean_activations.shape) # (n_layers, n_activations, num_last_tokens, d_model)\n",
    "print(no_thinking_activations_all.shape) # (n_layers, n_activations, batch_size, num_last_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_mean_activations_at_last_token = thinking_mean_activations[:, :, -1, :] # (n_layers, n_activations, d_model)\n",
    "thinking_mean_activations_normed = thinking_mean_activations_at_last_token / thinking_mean_activations_at_last_token.norm(dim=-1, keepdim=True)\n",
    "\n",
    "no_thinking_mean_activations_at_last_token = no_thinking_mean_activations[:, :, -1, :] # (n_layers, n_activations, d_model)\n",
    "no_thinking_mean_activations_normed = no_thinking_mean_activations_at_last_token / no_thinking_mean_activations_at_last_token.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(thinking_mean_activations_normed.shape) # (n_layers, n_activations, d_model)\n",
    "print(no_thinking_mean_activations_normed.shape) # (n_layers, n_activations, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16795406",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_refusal_vectors = thinking_mean_activations_normed.to(\"cpu\") - no_thinking_mean_activations_normed.to(\"cpu\")\n",
    "candidate_refusal_vectors_normed = candidate_refusal_vectors / candidate_refusal_vectors.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(candidate_refusal_vectors_normed.shape) # (n_layers, n_activations, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f875b",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7aff1",
   "metadata": {},
   "source": [
    "### Scalar projections of activations onto the local refusal direction at each extraction point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3cc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_map = {\n",
    "    \"thinking\": plotly.colors.qualitative.Plotly[0],\n",
    "    \"no_thinking\": plotly.colors.qualitative.Plotly[1],\n",
    "    \"neutral\": plotly.colors.qualitative.Pastel1[3],\n",
    "\n",
    "}\n",
    "colour_map_light = {\n",
    "    \"thinking\": plotly.colors.qualitative.Pastel1[1],\n",
    "    \"no_thinking\": plotly.colors.qualitative.Pastel1[0],\n",
    "    \"neutral\": plotly.colors.qualitative.Pastel1[3],\n",
    "}\n",
    "colour_map_opaque = {\n",
    "    \"thinking\": \"rgba(251, 180, 174, 0.3)\",\n",
    "    \"no_thinking\": \"rgba(179, 205, 227, 0.3)\",   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category2acts_normed = {\n",
    "    \"thinking\": thinking_activations_all.cpu(),\n",
    "    \"no_thinking\": no_thinking_activations_all.cpu(),\n",
    "} # (n_layers, n_activations, batch_size, num_last_tokens, d_model)\n",
    "\n",
    "\n",
    "x_values = [str(i) for i in range(2 * n_layers)]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for category in [\"thinking\", \"no_thinking\"]:\n",
    "    acts_normed = category2acts_normed[category][:, :, :, chosen_token_pos] # (n_layers, n_activations, batch_size, d_model)\n",
    "    projections = einops.einsum(\n",
    "        candidate_refusal_vectors_normed,\n",
    "        acts_normed,\n",
    "        \"layer act dim, layer act batch dim -> layer act batch\",\n",
    "    )\n",
    "    projections = torch.tensor(projections)\n",
    "\n",
    "    mean_projection = projections.mean(dim=-1) # layer act batch -> layer act\n",
    "\n",
    "    y_values = mean_projection.flatten() # layer act -> layer * act\n",
    "\n",
    "\n",
    "    # mean\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values,\n",
    "            y=y_values,\n",
    "            name=category,\n",
    "            mode=\"lines+markers\",\n",
    "            yaxis=\"y\",\n",
    "            marker=dict(color=colour_map[category], size=3),\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values,\n",
    "            y=y_values,\n",
    "            name=category,\n",
    "            mode=\"lines+markers\",\n",
    "            yaxis=\"y\",\n",
    "            marker=dict(color=colour_map_light[category], size=3),\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # variance\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values,\n",
    "            y=projections.reshape(-1, projections.shape[-1]),\n",
    "            name=category,\n",
    "            mode=\"lines+markers\",\n",
    "            yaxis=\"y\",\n",
    "            fillcolor=colour_map_opaque[category],\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # dot markers\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values[1::],\n",
    "            y=y_values[1::],\n",
    "            name=f\"{category}\",\n",
    "            mode=\"markers\",\n",
    "            yaxis=\"y\",\n",
    "            marker=dict(color=colour_map[category], size=3),\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    grid=dict(rows=1, columns=1),\n",
    "    xaxis=dict(\n",
    "        type=\"category\",\n",
    "        title=dict(text=\"Extraction Point\", font=dict(size=20)),\n",
    "        dtick=4,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        tickfont=dict(size=18),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=\"Scalar Projection\", font=dict(size=20)),\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "        tickfont=dict(size=18),\n",
    "    ),\n",
    "    hovermode=\"x unified\",\n",
    "    height=250,\n",
    "    width=600,\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    legend=dict(x=0.05, y=0.95, font=dict(size=18)),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"prj_onto_local_candidate_refusal_vectors.pdf\", scale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df74da9",
   "metadata": {},
   "source": [
    "### Mean cosine of refusal directions at each layer with at other layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_dirs = candidate_refusal_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842054df",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = sum([[f\"{i}\", f\"{i}-post\"] for i in range(n_layers)], [])\n",
    "layer_names = [str(i) for i in range(2 * n_layers)]\n",
    "\n",
    "\n",
    "refusal_dirs_flatten = refusal_dirs.reshape((-1, refusal_dirs.shape[-1]))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=layer_names,\n",
    "        y=refusal_dirs_flatten.norm(dim=-1),\n",
    "        mode=\"lines+markers\",\n",
    "        yaxis=\"y\",\n",
    "        marker_color=colour_map_light[\"neutral\"],\n",
    "        marker_size=8,\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=layer_names[::],\n",
    "        y=refusal_dirs_flatten.norm(dim=-1)[::],\n",
    "        mode=\"markers\",\n",
    "        yaxis=\"y\",\n",
    "        marker_color=colour_map[\"neutral\"],\n",
    "        marker_size=8,\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(layer_names[np.argmax(refusal_dirs_flatten.norm(dim=-1)[:-1])])\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=(\n",
    "    #     \"Statistics of refusal direction candidates at each layer\"\n",
    "    #     f\" layer for {MODEL_PATH}\"\n",
    "    # ),\n",
    "    plot_bgcolor=\"white\",\n",
    "    grid=dict(rows=1, columns=1),\n",
    "    xaxis=dict(\n",
    "        type=\"category\",\n",
    "        title=dict(text=\"Extraction Point\", font=dict(size=28)),\n",
    "        dtick=4,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        tickfont=dict(size=24),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=\"Norm of<br>Refusal Direction\", font=dict(size=28)),\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "        tickfont=dict(size=24),\n",
    "    ),\n",
    "    hovermode=\"x unified\",\n",
    "    height=300,\n",
    "    width=1000,\n",
    "    # width=20 + 12 * len(x_values),\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# fig.write_image(VISUALIZATION_DIR / \"norm_refusal.pdf\", scale=5)\n",
    "\n",
    "\n",
    "flatten_dirs = refusal_dirs.reshape(-1, refusal_dirs.shape[-1])\n",
    "pairwise_cosine = flatten_dirs @ flatten_dirs.T\n",
    "# pairwise_cosine = np.arccos(pairwise_cosine)\n",
    "mean_cosine = np.nanmean(pairwise_cosine, axis=-1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=layer_names,\n",
    "        y=mean_cosine,\n",
    "        mode=\"lines+markers\",\n",
    "        yaxis=\"y\",\n",
    "        marker_color=colour_map_light[\"neutral\"],\n",
    "        showlegend=False,\n",
    "        marker_size=8,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=layer_names[::],\n",
    "        y=mean_cosine[::],\n",
    "        mode=\"markers\",\n",
    "        yaxis=\"y\",\n",
    "        marker_color=colour_map[\"neutral\"],\n",
    "        showlegend=False,\n",
    "        marker_size=8,\n",
    "    )\n",
    ")\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=layer_names,\n",
    "#         y=raw_dirs.norm(dim=-1) + mean_cosine / mean_cosine.max(),\n",
    "#         mode=\"lines+markers\",\n",
    "#         yaxis=\"y3\",\n",
    "#         marker_color=colour_map_light[\"neutral\"],\n",
    "#         showlegend=False\n",
    "#     )\n",
    "# )\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=(\n",
    "    #     \"Statistics of refusal direction candidates at each extraction point\"\n",
    "    #     f\" for {MODEL_PATH}\"\n",
    "    # ),\n",
    "    plot_bgcolor=\"white\",\n",
    "    grid=dict(rows=1, columns=1),\n",
    "    xaxis=dict(\n",
    "        type=\"category\",\n",
    "        title=dict(text=\"Extraction Point\", font=dict(size=28)),\n",
    "        dtick=4,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        tickfont=dict(size=24),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=f\"Mean<br>Cosine Score\", font=dict(size=28)),\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "        tickfont=dict(size=24),\n",
    "    ),\n",
    "    hovermode=\"x unified\",\n",
    "    height=300,\n",
    "    width=1000,\n",
    "    # width=20 + 12 * len(x_values),\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "layer_names[np.nanargmax(mean_cosine)]\n",
    "\n",
    "\n",
    "# fig.write_image(VISUALIZATION_DIR / \"mean_cosine.pdf\", scale=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
