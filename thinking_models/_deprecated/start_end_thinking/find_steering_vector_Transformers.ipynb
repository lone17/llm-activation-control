{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fac789c",
   "metadata": {},
   "source": [
    "In this notebook, we get the constrastive sets of activations by creating checkpoints at start think and end think tokens:\n",
    "\n",
    "```md\n",
    "thinking:\n",
    "\tWhat is LLM?<think>\\n\\nOkay the user ask...</think>\\n\\nLLM is...\n",
    "\t\t\t^                               ^\n",
    "\t\t        Here                            Here\n",
    "```\n",
    "\n",
    "The model is loaded to Transformer's AutoModelCausalLM instead of HookedTransformer as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1dc7d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ea6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers transformers_stream_generator tiktoken einops jaxtyping colorama kaleido numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b49881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import TypedDict, Literal\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import Callable\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from jaxtyping import Float, Int\n",
    "from enum import StrEnum\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and the model\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_instructions_qwen_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: list[str],\n",
    "    enable_thinking: bool = True,\n",
    ") -> Int[Tensor, \"batch_size seq_len\"]:\n",
    "    prompts = []\n",
    "    for instruction in instructions:\n",
    "        message = [{\"role\": \"user\", \"content\": instruction}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "            enable_thinking=enable_thinking,\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(\n",
    "    tokenize_instructions_qwen_chat,\n",
    "    tokenizer=tokenizer,\n",
    "    enable_thinking=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9102a9",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_instructions() -> tuple[list[str], list[str]]:\n",
    "    url = \"https://raw.githubusercontent.com/cvenhoff/steering-thinking-llms/refs/heads/main/messages/messages.py\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    # Save to file\n",
    "    with open(\"messages.py\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    # Load the messages\n",
    "    assert Path(\"messages.py\").exists()\n",
    "    from messages import messages, eval_messages\n",
    "\n",
    "    train_contents = [msg[\"content\"] for msg in messages]\n",
    "    eval_contents = [msg[\"content\"] for msg in eval_messages]\n",
    "\n",
    "    # Shuffle the messages\n",
    "    random.shuffle(train_contents)\n",
    "    random.shuffle(eval_contents)\n",
    "\n",
    "    return train_contents, eval_contents\n",
    "\n",
    "def preprocess_instructions(instructions: list[str]) -> list[str]:\n",
    "    return [ins + \" Think fast and briefly.\" for ins in instructions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_train, instructions_test = get_dataset_instructions()\n",
    "instructions_train = preprocess_instructions(instructions_train)\n",
    "instructions_test = preprocess_instructions(instructions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "MAX_NEW_TOKENS = -1\n",
    "UPPERBOUND_MAX_NEW_TOKENS = 7000\n",
    "\n",
    "start_thinking_token_id = 151667\n",
    "end_thinking_token_id = 151668 # Qwen3's end of thinking token id\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_activations = 2\n",
    "d_model = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdfdbf",
   "metadata": {},
   "source": [
    "## Compute activations & refusal dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_tokens_activations(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    tokens: torch.Tensor,\n",
    "    target_tokens: list[str] = [\"<think>\", \"</think>\"],\n",
    "    max_new_tokens: int = -1,\n",
    "    module_names: list[str] = [\"input_layernorm\", \"post_attention_layernorm\", \"post_feedforward_layernorm\"],\n",
    "    layers: list[int] | None = None,\n",
    "    offload_to_cpu: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tokens and extract activations only when target tokens are generated.\n",
    "    \n",
    "    This approach uses a two-phase strategy:\n",
    "    1. Fast generation using model.generate() to get all tokens\n",
    "    2. Selective activation extraction only for target token positions\n",
    "    \n",
    "    This is much faster than manual token-by-token generation because:\n",
    "    - model.generate() uses KV caching for efficiency\n",
    "    - We only do expensive activation extraction when needed\n",
    "    \n",
    "    Returns:\n",
    "        token_activations: {token_str: {module_name: activation_tensor}}\n",
    "        final_tokens: the complete generated sequence\n",
    "    \"\"\"\n",
    "    if layers is None:\n",
    "        # Get number of layers from model config\n",
    "        if hasattr(model.config, 'num_hidden_layers'):\n",
    "            num_layers = model.config.num_hidden_layers\n",
    "        elif hasattr(model.config, 'n_layers'):\n",
    "            num_layers = model.config.n_layers\n",
    "        else:\n",
    "            raise ValueError(\"Could not determine number of layers from model config\")\n",
    "        layers = list(range(num_layers))\n",
    "\n",
    "    if max_new_tokens == -1:\n",
    "        max_new_tokens = UPPERBOUND_MAX_NEW_TOKENS\n",
    "\n",
    "    # Convert target tokens to token IDs\n",
    "    target_token_ids = set()\n",
    "    target_token_to_str = {}\n",
    "\n",
    "    for token_str in target_tokens:\n",
    "        token_ids = tokenizer(token_str, return_tensors=\"pt\").input_ids.squeeze()\n",
    "        if token_ids.dim() == 0:\n",
    "            token_ids = [token_ids.item()]\n",
    "        else:\n",
    "            token_ids = token_ids.tolist()\n",
    "        \n",
    "        if len(token_ids) == 1:\n",
    "            target_token_ids.add(token_ids[0])\n",
    "            target_token_to_str[token_ids[0]] = token_str\n",
    "        else:\n",
    "            print(f\"Warning: {token_str} tokenizes to multiple tokens: {token_ids}\")\n",
    "            # Handle multi-token case - take the last token as trigger\n",
    "            target_token_ids.add(token_ids[-1])\n",
    "            target_token_to_str[token_ids[-1]] = token_str\n",
    "\n",
    "    print(f\"Monitoring token IDs: {target_token_ids}\")\n",
    "    print(f\"Token mapping: {target_token_to_str}\")\n",
    "\n",
    "    # Move tokens to model device\n",
    "    tokens = tokens.to(model.device)\n",
    "    \n",
    "    # Phase 1: Fast generation using model.generate()\n",
    "    start_time = time.time()\n",
    "    print(\"Phase 1: Fast generation using model.generate()...\")\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    print(f\"Max new tokens: {max_new_tokens}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=tokens,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,  # We don't need scores, saves memory\n",
    "        )\n",
    "    \n",
    "    # Extract the full sequence\n",
    "    full_sequence = generated_tokens.sequences[0]  # Remove batch dimension\n",
    "    new_tokens = full_sequence[tokens.shape[1]:]  # Only the newly generated part\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "\n",
    "    # ==================================\n",
    "    # Phase 2: Find target token positions\n",
    "    start_time = time.time()\n",
    "    print(\"Phase 2: Finding target token positions...\")\n",
    "    target_positions = []\n",
    "    for i, token_id in enumerate(new_tokens):\n",
    "        if token_id.item() not in target_token_ids:\n",
    "            continue\n",
    "        # absolute position including original prompt\n",
    "        absolute_position = tokens.shape[1] + i\n",
    "        token_str = target_token_to_str[token_id.item()]\n",
    "        target_positions.append((absolute_position, token_id.item(), token_str))\n",
    "        print(f\"Found target token '{token_str}' at position {absolute_position}\")\n",
    "    \n",
    "    print(f\"Found {len(target_positions)} target tokens\")\n",
    "\n",
    "    target_token_position_time = time.time() - start_time\n",
    "    print(f\"Target token position time: {target_token_position_time:.2f} seconds\")\n",
    "\n",
    "    # ==================================\n",
    "    # Phase 3: Extract activations only for target positions\n",
    "    start_time = time.time()\n",
    "    print(\"Phase 3: Extracting activations for target positions...\")\n",
    "    token_activations: dict[str, dict[str, Tensor]] = {\n",
    "        token_str: {} for token_str in target_tokens\n",
    "    }\n",
    "    \n",
    "    if not target_positions:\n",
    "        print(\"No target tokens found, returning empty activations\")\n",
    "        return token_activations, full_sequence.unsqueeze(0)\n",
    "    \n",
    "    # Get target modules\n",
    "    def get_target_modules():\n",
    "        target_modules = []\n",
    "        for layer in layers:\n",
    "            for module_name in module_names:\n",
    "                # Convert module names to actual modules\n",
    "                module_path = f\"model.layers.{layer}.{module_name}\"\n",
    "                \n",
    "                # Get the actual module\n",
    "                try:\n",
    "                    module = model\n",
    "                    for part in module_path.split('.'):\n",
    "                        module = getattr(module, part)\n",
    "                    target_modules.append((module, module_path))\n",
    "                except AttributeError:\n",
    "                    print(f\"Warning: Could not find module {module_path}\")\n",
    "                    continue\n",
    "        return target_modules\n",
    "\n",
    "    target_modules = get_target_modules()\n",
    "    \n",
    "    # Extract activations for each target position\n",
    "    for position, token_id, token_str in tqdm(target_positions, desc=\"Extracting activations\"):\n",
    "        # Get sequence up to and including this position\n",
    "        seq_up_to_pos = full_sequence[:position + 1].unsqueeze(0)  # Add batch dim\n",
    "        \n",
    "        # Set up activation cache for this position\n",
    "        step_activations = {}\n",
    "        \n",
    "        # Create hooks for activation extraction\n",
    "        hook_handles = []\n",
    "        for module, module_key in target_modules:\n",
    "            def make_hook(key, target_pos=position):\n",
    "                def hook_fn(module, input, output):\n",
    "                    # Extract activation at the target position\n",
    "                    if hasattr(output, 'shape') and len(output.shape) >= 3:\n",
    "                        activation = output[:, target_pos, :].clone()  # Keep batch and single token dim\n",
    "                    else:\n",
    "                        activation = output.clone()\n",
    "                    \n",
    "                    if offload_to_cpu:\n",
    "                        activation = activation.cpu()\n",
    "                    step_activations[key] = activation\n",
    "                return hook_fn\n",
    "            \n",
    "            handle = module.register_forward_hook(make_hook(module_key, position))\n",
    "            hook_handles.append(handle)\n",
    "        \n",
    "        # Forward pass with hooks to extract activations\n",
    "        with torch.no_grad():\n",
    "            _ = model(seq_up_to_pos)\n",
    "        \n",
    "        # Remove hooks\n",
    "        for handle in hook_handles:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Store activations for this token\n",
    "        for module_key, activation in step_activations.items():\n",
    "            if token_str not in token_activations:\n",
    "                token_activations[token_str] = {}\n",
    "            \n",
    "            # If we already have activations for this token type, concatenate them\n",
    "            if module_key in token_activations[token_str]:\n",
    "                print(f\"Module key {module_key} exists, let concat the activations\")\n",
    "                token_activations[token_str][module_key] = torch.cat([\n",
    "                    token_activations[token_str][module_key], \n",
    "                    activation\n",
    "                ], dim=1)  # Concatenate along token dimension\n",
    "            else:\n",
    "                token_activations[token_str][module_key] = activation\n",
    "        \n",
    "        # Clean up\n",
    "        del step_activations, hook_handles\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    activation_extraction_time = time.time() - start_time\n",
    "    print(f\"Activation extraction time: {activation_extraction_time:.2f} seconds\")\n",
    "    \n",
    "    return token_activations, full_sequence.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_positive_caches = []\n",
    "batch_negative_caches = []\n",
    "\n",
    "subset_instructions = instructions_train[:TRAIN_BATCH_SIZE]\n",
    "for i, instruction in enumerate(subset_instructions):\n",
    "    print(f\"Processing instruction {i}:\")\n",
    "    tokens = tokenize_instructions_fn(instructions=[instruction])\n",
    "    print(f\"Number of tokens: {tokens.shape[1]}\")\n",
    "    token_activations, tokens = get_generated_tokens_activations(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens,\n",
    "        max_new_tokens=-1,\n",
    "        module_names=[\"input_layernorm\", \"post_attention_layernorm\"],\n",
    "        target_tokens=[\"<think>\", \"</think>\"],\n",
    "    )\n",
    "    batch_positive_caches.append(token_activations[\"<think>\"])\n",
    "    batch_negative_caches.append(token_activations[\"</think>\"])\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97809049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the activations for backup\n",
    "\n",
    "def save_tensor(\n",
    "    tensor: Tensor,\n",
    "    save_dir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    save_path = save_dir / f\"{name}.pt\"\n",
    "    torch.save(tensor, save_path)\n",
    "    print(f\"Saved {name} to {save_path}\")\n",
    "\n",
    "output_dir = Path(\"/root/workspace/outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "save_tensor(\n",
    "    batch_positive_caches,\n",
    "    output_dir,\n",
    "    \"batch_positive_caches.pt\",\n",
    ")\n",
    "save_tensor(\n",
    "    batch_negative_caches,\n",
    "    output_dir,\n",
    "    \"batch_negative_caches.pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ea7b2",
   "metadata": {},
   "source": [
    "### Compute refusal directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_path(\n",
    "    layer: int,\n",
    "    module_name: list[str] = [\"input_layernorm\", \"post_attention_layernorm\"]\n",
    "):\n",
    "    return f\"model.layers.{layer}.{module_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe692da",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_mean_activations = torch.zeros(\n",
    "    n_layers, n_activations, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, d_model)\n",
    "\n",
    "positive_activations_all = torch.zeros(\n",
    "    n_layers, n_activations, 29, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, batch_size, d_model)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    batch_positive_activations = []\n",
    "    for module_name in [\"input_layernorm\", \"post_attention_layernorm\"]:\n",
    "        # Get mean activations across tokens dimension\n",
    "        batch_positive_activations_per_module = []\n",
    "        for i, sample in enumerate(batch_positive_caches):\n",
    "            batch_positive_activations_per_module.append(\n",
    "                sample[get_module_path(layer, module_name)][0,:]\n",
    "            )\n",
    "        batch_positive_activations_per_module = torch.stack(batch_positive_activations_per_module)\n",
    "        batch_positive_activations.append(batch_positive_activations_per_module)\n",
    "    batch_positive_activations = torch.stack(batch_positive_activations) # (n_activations, batch_size, d_model)\n",
    "\n",
    "    # Normalize then get mean because the activation will be normalized by the RMSNorm layer\n",
    "    batch_positive_activations = batch_positive_activations / batch_positive_activations.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute mean across batch dimension\n",
    "    positive_mean_activations[layer] = batch_positive_activations.mean(dim=1)\n",
    "    positive_activations_all[layer] = batch_positive_activations\n",
    "\n",
    "print(positive_mean_activations.shape) # (n_layers, n_activations, num_last_tokens, d_model)\n",
    "print(positive_activations_all.shape) # (n_layers, n_activations, batch_size, num_last_tokens, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mean_activations = torch.zeros(\n",
    "    n_layers, n_activations, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, d_model)\n",
    "\n",
    "negative_activations_all = torch.zeros(\n",
    "    n_layers, n_activations, 29, d_model, device=DEVICE, dtype=torch.float16\n",
    ") # (n_layers, n_activations, batch_size, d_model)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    batch_negative_activations = []\n",
    "    for module_name in [\"input_layernorm\", \"post_attention_layernorm\"]:\n",
    "        # Get mean activations across tokens dimension\n",
    "        batch_negative_activations_per_module = []\n",
    "        for i, sample in enumerate(batch_negative_caches):\n",
    "            batch_negative_activations_per_module.append(\n",
    "                sample[get_module_path(layer, module_name)][0,:]\n",
    "            )\n",
    "        batch_negative_activations_per_module = torch.stack(batch_negative_activations_per_module)\n",
    "        batch_negative_activations.append(batch_negative_activations_per_module)\n",
    "    batch_negative_activations = torch.stack(batch_negative_activations) # (n_activations, batch_size, d_model)\n",
    "\n",
    "    # Normalize then get mean because the activation will be normalized by the RMSNorm layer\n",
    "    batch_negative_activations = batch_negative_activations / batch_negative_activations.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute mean across batch dimension\n",
    "    negative_mean_activations[layer] = batch_negative_activations.mean(dim=1)\n",
    "    negative_activations_all[layer] = batch_negative_activations\n",
    "\n",
    "print(negative_mean_activations.shape) # (n_layers, n_activations, d_model)\n",
    "print(negative_activations_all.shape) # (n_layers, n_activations, batch_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfa324",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_mean_activation_normed = positive_mean_activations / positive_mean_activations.norm(dim=-1, keepdim=True)\n",
    "negative_mean_activation_normed = negative_mean_activations / negative_mean_activations.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(positive_mean_activation_normed.shape) # (n_layers, n_activations, d_model)\n",
    "print(negative_mean_activation_normed.shape) # (n_layers, n_activations, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_refusal_vectors = positive_mean_activation_normed.to(\"cpu\") - negative_mean_activation_normed.to(\"cpu\")\n",
    "candidate_refusal_vectors_normed = candidate_refusal_vectors / candidate_refusal_vectors.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(candidate_refusal_vectors_normed.shape) # (n_layers, n_activations, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6587ba",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_map = {\n",
    "    \"positive\": plotly.colors.qualitative.Plotly[0],\n",
    "    \"negative\": plotly.colors.qualitative.Plotly[1],\n",
    "    \"neutral\": plotly.colors.qualitative.Pastel1[3],\n",
    "\n",
    "}\n",
    "colour_map_light = {\n",
    "    \"positive\": plotly.colors.qualitative.Pastel1[1],\n",
    "    \"negative\": plotly.colors.qualitative.Pastel1[0],\n",
    "    \"neutral\": plotly.colors.qualitative.Pastel1[3],\n",
    "}\n",
    "colour_map_opaque = {\n",
    "    \"positive\": \"rgba(251, 180, 174, 0.3)\",\n",
    "    \"negative\": \"rgba(179, 205, 227, 0.3)\",   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "category2acts_normed = {\n",
    "    \"positive\": positive_activations_all.cpu(),\n",
    "    \"negative\": negative_activations_all.cpu(),\n",
    "} # (n_layers, n_activations, batch_size, num_last_tokens, d_model)\n",
    "\n",
    "\n",
    "x_values = [str(i) for i in range(2 * n_layers)]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for category in [\"positive\", \"negative\"]:\n",
    "    acts_normed = category2acts_normed[category] # (n_layers, n_activations, batch_size, d_model)\n",
    "    projections = einops.einsum(\n",
    "        candidate_refusal_vectors_normed,\n",
    "        acts_normed,\n",
    "        \"layer act dim, layer act batch dim -> layer act batch\",\n",
    "    )\n",
    "    projections = torch.tensor(projections)\n",
    "\n",
    "    mean_projection = projections.mean(dim=-1) # layer act batch -> layer act\n",
    "\n",
    "    y_values = mean_projection.flatten() # layer act -> layer * act\n",
    "\n",
    "\n",
    "    # mean\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values,\n",
    "            y=y_values,\n",
    "            name=category,\n",
    "            mode=\"lines+markers\",\n",
    "            yaxis=\"y\",\n",
    "            marker=dict(color=colour_map[category], size=3),\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values,\n",
    "            y=y_values,\n",
    "            name=category,\n",
    "            mode=\"lines+markers\",\n",
    "            yaxis=\"y\",\n",
    "            marker=dict(color=colour_map_light[category], size=3),\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # variance\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values,\n",
    "            y=projections.reshape(-1, projections.shape[-1]),\n",
    "            name=category,\n",
    "            mode=\"lines+markers\",\n",
    "            yaxis=\"y\",\n",
    "            fillcolor=colour_map_opaque[category],\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # dot markers\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_values[1::],\n",
    "            y=y_values[1::],\n",
    "            name=f\"{category}\",\n",
    "            mode=\"markers\",\n",
    "            yaxis=\"y\",\n",
    "            marker=dict(color=colour_map[category], size=3),\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    grid=dict(rows=1, columns=1),\n",
    "    xaxis=dict(\n",
    "        type=\"category\",\n",
    "        title=dict(text=\"Extraction Point\", font=dict(size=20)),\n",
    "        dtick=4,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        tickfont=dict(size=18),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=\"Scalar Projection\", font=dict(size=20)),\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "        tickfont=dict(size=18),\n",
    "    ),\n",
    "    hovermode=\"x unified\",\n",
    "    height=250,\n",
    "    width=600,\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    legend=dict(x=0.05, y=0.95, font=dict(size=18)),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# fig.write_image(\"prj_onto_local_candidate_refusal_vectors.pdf\", scale=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
