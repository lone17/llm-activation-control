{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_memory_tracking():\n",
    "    \"\"\"Initialize GPU memory tracking.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    else:\n",
    "        print(\"This notebook is intended for CUDA GPUs but CUDA is not available.\")\n",
    "\n",
    "\n",
    "def print_memory_usage():\n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated() / (\n",
    "        1024**3\n",
    "    )  # Convert bytes to GB\n",
    "    print(f\"Maximum GPU memory allocated: {max_gpu_memory:.1f} GB\")\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(3)  # some buffer time to allow memory to clear\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    max_memory_allocated = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
    "    print(f\"Maximum GPU memory allocated: {max_memory_allocated:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def monitor_memory_usage_in_gb(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process(os.getpid())\n",
    "\n",
    "        # Measure the baseline memory usage before running the function\n",
    "        baseline_mem = process.memory_info().rss / 1024**3  # in GB\n",
    "\n",
    "        # Start monitoring memory in a separate thread\n",
    "        mem_usage = []\n",
    "        done = False\n",
    "\n",
    "        def monitor_memory():\n",
    "            while not done:\n",
    "                mem_usage.append(process.memory_info().rss / 1024**3)  # Convert to GB\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        t = Thread(target=monitor_memory)\n",
    "        t.start()\n",
    "\n",
    "        # Run the function\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Stop monitoring\n",
    "        done = True\n",
    "        t.join()\n",
    "\n",
    "        peak_mem_usage_gb = max(mem_usage) - baseline_mem\n",
    "        print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@monitor_memory_usage_in_gb\n",
    "def load_model(model_name, device):\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    with torch.device(\"meta\"):\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"model.pth\", map_location=device, weights_only=True, mmap=True),\n",
    "        assign=True,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.loading_from_pretrained import OFFICIAL_MODEL_NAMES\n",
    "\n",
    "if model_name not in OFFICIAL_MODEL_NAMES:\n",
    "    OFFICIAL_MODEL_NAMES.append(model_name)\n",
    "\n",
    "hooked_model = HookedTransformer.from_pretrained_no_processing(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    # dtype=torch.bfloat16,\n",
    "    default_padding_side=\"left\",\n",
    "    # bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.num_key_value_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(hooked_model.blocks[0].attn.b_Q)\n",
    "torch.allclose(\n",
    "    hooked_model.blocks[0].attn.b_Q.data.view(-1),\n",
    "    model.model.layers[0].self_attn.q_proj.bias.data,\n",
    ")\n",
    "\n",
    "model.model.layers[0].self_attn.o_proj.weight.data.shape\n",
    "print(dir(hooked_model.blocks[0].attn))\n",
    "print(hooked_model.blocks[0].attn.W_O.shape)\n",
    "print(hooked_model.blocks[0].mlp.W_out.shape)\n",
    "dir(model.model.layers[0].mlp)\n",
    "# model.model.layers[0].self_attn.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "num_layers = len(hooked_model.blocks) * 2\n",
    "mean = [[0] * num_layers for _ in range(num_layers)]\n",
    "std = [[0] * num_layers for _ in range(num_layers)]\n",
    "\n",
    "for i in range(num_layers):\n",
    "    if i % 2 == 0:\n",
    "        A = hooked_model.blocks[i // 2].attn.W_O\n",
    "    else:\n",
    "        A = hooked_model.blocks[i // 2].mlp.W_out\n",
    "    for j in range(num_layers):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if j % 2 == 0:\n",
    "            B = hooked_model.blocks[j // 2].attn.W_O\n",
    "        else:\n",
    "            B = hooked_model.blocks[j // 2].mlp.W_out\n",
    "        m = A @ B.T\n",
    "        mean[i][j] = m.mean()\n",
    "        std[i][j] = m.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-activation-control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
